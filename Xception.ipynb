{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "Kopia notatnika Xception.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMEjwiUylp69"
      },
      "source": [
        "# Transfer Learning\n",
        "This project should introduce you to transfer learning techniques. The checklist below shows the steps necessary to finish the project.\n",
        "- **Prepare a dataset**. The dataset shall be desined for image classification, and be organized to work with keras.applications (training, validation and test data should be in different folders. Images from one class should be in one subfolder). \n",
        "- **Select the network**. Select the network you want to work with. In the original code `Xception` network is selected, but you may change it if you want.\n",
        "- **Complete functions**. Complete missing code in the following functions\n",
        "    - `add_new_last_layer` - adds an output layer to your model. You may also add some layers in between. If so, remember to unfreeze these layers in `setup_to_transfer_learn` function\n",
        "    - `setup_to_transfer_learn` - Freezes all layers but the last one and compiles the model\n",
        "    - `setup_to_finetune` - Freezes the first not_trainable layers, and unfreezes the rest\n",
        "    - `train_generator` - Creates a generator feed with train dataset. You should add data augmentation to it\n",
        "- **Perorm training**. Perform model trainig slowly unfreezeng the layers as long as you do not observe overfitting. In case of overfitting get back to previous model.\n",
        "- **Test your model on real images**. Write a code that tests your model on real images. You should display an image, a real class of an image, the class selected by the model, and the probabilities assined by the model to all the classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLtWaKgdxPWI"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import glob\n",
        "\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.keras.applications.xception import Xception, preprocess_input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import Nadam\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4dVAqoqSxD7q"
      },
      "source": [
        "# Run if you are using google drive\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNospTanxyWV"
      },
      "source": [
        "# Run to see how to access your google drive\n",
        "# !ls \"/content/drive/My Drive/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDpbsBT2xPWL"
      },
      "source": [
        "# If you have more than 1 GPU and you want to train on the first one only\n",
        "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "csZ0XAgAxPWM"
      },
      "source": [
        "def get_nb_files(directory):\n",
        "    \"\"\"Get number of files by searching directory recursively\"\"\"\n",
        "    if not os.path.exists(directory):\n",
        "        print ('No directory')\n",
        "        return 0\n",
        "    cnt = 0\n",
        "    for r, dirs, files in os.walk(directory):\n",
        "        for dr in dirs:\n",
        "              cnt += len(glob.glob(os.path.join(r, dr + \"/*\")))\n",
        "    return cnt\n",
        "\n",
        "def add_new_last_layer(base_model, nb_classes):\n",
        "    \"\"\"Add last layer to the convnet\n",
        "    Arguments:\n",
        "        base_model: keras model (without a top layer)\n",
        "        nb_classes: number of classes\n",
        "    Returns:\n",
        "        new keras model with last layer\n",
        "    \"\"\"\n",
        "    x = base_model.output\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    # Add the last layer and ((optionally) a dense layer\n",
        "    # ENTER YOUR CODE HERE \n",
        "    \n",
        "    #END OF YOUR CODE\n",
        "    model = Model(inputs=base_model.input, outputs=predictions)\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "def setup_to_transfer_learn(model, args):\n",
        "    \"\"\"Freezes all layers but the last one and compiles the model\"\"\"\n",
        "    \n",
        "    # TODO: Set all the layers but last one to not trainable.\n",
        "    # Note: If you added more than one layer in the add_new_last_layer \n",
        "    # function you should set all these layers to trainable.\n",
        "    #ENTER YOUR CODE HERE    \n",
        "\n",
        "    #END OF YOUR CODE\n",
        "\n",
        "    model.layers[-1].trainable=True\n",
        "    opt = Nadam(lr=args.lr)\n",
        "    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "def setup_to_finetune(model, args, not_trainable):\n",
        "    \"\"\"Freezes the first not_trainable layers, and unfreezes the rest\n",
        "    Arguments:\n",
        "        model: keras model\n",
        "        not_trainable: number of not trainable layers\n",
        "    \"\"\"\n",
        "    \n",
        "    #Set the layers [0:not_trainable] to not trainable. Set the layers [not_trainable:] to trainable\n",
        "    #ENTER YOUR CODE HERE\n",
        "\n",
        "    #END OF YOUR CODE\n",
        "\n",
        "    opt = Nadam(lr=args.lr)\n",
        "    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    \n",
        "def trainable_params(model, output_format = 'en_US'):\n",
        "    \"\"\"Computes the number of trainable parameters in the model.\n",
        "    Args:\n",
        "        model: keras model\n",
        "        output_format: (default: 'en_US')\n",
        "            'int'   - number of trainable parameters (12345)\n",
        "            'en_US' - number of trainable parameters formated to comma separated str ('12,235')\n",
        "    \"\"\"    \n",
        "    \n",
        "    def nr_to_string(number, separator = ','):\n",
        "        \"\"\"\n",
        "        Changes numbers to string.\n",
        "        \"\"\"\n",
        "        s = '%d' % number\n",
        "        groups = []\n",
        "        while s and s[-1].isdigit():\n",
        "            groups.append(s[-3:])\n",
        "            s = s[:-3]\n",
        "        return s + separator.join(reversed(groups))\n",
        "\n",
        "    # And the real trainable_params function\n",
        "    \n",
        "    ret = sum([np.prod(K.get_value(w).shape) for w in model.trainable_weights])\n",
        "    if output_format == 'en_US':\n",
        "        return nr_to_string(ret)\n",
        "    else:\n",
        "        return ret"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7ikt-GBxPWN"
      },
      "source": [
        "\n",
        "### Plot results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gcu0Gl27xPWN"
      },
      "source": [
        "def unpack_history(history, old_history = None):\n",
        "    if old_history is None:\n",
        "        new_history = {\n",
        "            'accuracy' : [],\n",
        "            'val_accuracy' : [],\n",
        "            'loss' : [],\n",
        "            'val_loss' : [],\n",
        "        }  \n",
        "    else:\n",
        "        new_history = old_history\n",
        "    new_history['accuracy'] += history.history['accuracy']\n",
        "    new_history['val_accuracy'] += history.history['val_accuracy']\n",
        "    new_history['loss'] += history.history['loss']\n",
        "    new_history['val_loss'] += history.history['val_loss']\n",
        "    return new_history\n",
        "\n",
        "def plot_history(training_history):\n",
        "    acc = training_history['accuracy']\n",
        "    val_acc = training_history['val_accuracy']\n",
        "    loss = training_history['loss']\n",
        "    val_loss = training_history['val_loss']\n",
        "    epochs = np.arange(len(acc)) + 1\n",
        "    \n",
        "    fig = plt.figure(figsize=(12, 4))\n",
        "\n",
        "    ax1 = fig.add_subplot(121)    \n",
        "    ax1.plot(epochs, loss, c='g', label='Train')\n",
        "    ax1.plot(epochs, val_loss, c='r', label='Valid')\n",
        "    ax1.set_title('Loss')\n",
        "    ax1.legend(loc='lower left');\n",
        "    ax1.grid(True)\n",
        "    \n",
        "    ax2 = fig.add_subplot(122)    \n",
        "    ax2.plot(epochs, acc, c='g', label='Train')\n",
        "    ax2.plot(epochs, val_acc, c='r', label='Valid')\n",
        "    ax2.set_title('Accuracy')\n",
        "    #ax2.legend(loc='upper left');\n",
        "    ax2.grid(True)\n",
        "        \n",
        "    plt.show()    \n",
        "\n",
        "\n",
        "def process_and_display(history, old_history = None):\n",
        "    new_history = unpack_history(history, old_history)\n",
        "    plot_history(new_history)\n",
        "    return new_history\n",
        "    \n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIYqpeLlxPWO"
      },
      "source": [
        "### Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27_24vQMxPWO"
      },
      "source": [
        "class Args():\n",
        "    def __init__(self):\n",
        "        \n",
        "        self.im_width, self.im_height = 299, 299 #fixed size for Xception\n",
        "        self.lr = 0.001\n",
        "        self.batch_size = 128\n",
        "        \n",
        "        local_folder = './data'\n",
        "        self.train_dir = os.path.join(local_folder, 'seg_train/')\n",
        "        self.valid_dir = os.path.join(local_folder, 'seg_valid/')\n",
        "        self.test_dir = os.path.join(local_folder, 'seg_test/')\n",
        "            \n",
        "args = Args()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PTivQAhmxPWO"
      },
      "source": [
        "### Prepare global variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eGeRrcsvxPWP"
      },
      "source": [
        "nb_train_samples = get_nb_files(args.train_dir)\n",
        "classes = glob.glob(args.train_dir + \"/*\")\n",
        "classes = [x.split('/')[-1] for x in classes]\n",
        "classes.sort()\n",
        "nb_classes = len(classes)\n",
        "nb_valid_samples = get_nb_files(args.valid_dir)\n",
        "\n",
        "train_steps = int(nb_train_samples / args.batch_size)\n",
        "valid_steps = int (nb_valid_samples / args.batch_size)\n",
        "\n",
        "print ('Train dataset contains {} samples ({} steps / epoch)'.format(nb_train_samples, train_steps))\n",
        "print ('Valid dataset contains {} samples ({} steps / epoch)'.format(nb_valid_samples, valid_steps))\n",
        "print ('Dataset contains {} classes ({}).'.format(nb_classes, classes))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRa5FjkHxPWP"
      },
      "source": [
        "### Generators"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1fRcyIqxPWQ"
      },
      "source": [
        "# add data augmentation to the generator\n",
        "train_datagen = ImageDataGenerator(\n",
        "    preprocessing_function=preprocess_input,\n",
        "    #ENTER YOUR CODE\n",
        "\n",
        "    #END OF YOUR CODE\n",
        ")\n",
        "\n",
        "test_datagen = ImageDataGenerator(\n",
        "    preprocessing_function=preprocess_input,\n",
        ")\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    args.train_dir,\n",
        "    target_size=(args.im_width, args.im_height),\n",
        "    batch_size=args.batch_size,\n",
        ")\n",
        "\n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "    args.valid_dir,\n",
        "    target_size=(args.im_width, args.im_height),\n",
        "    batch_size=args.batch_size,\n",
        ")\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    args.test_dir,\n",
        "    target_size=(args.im_width, args.im_height),\n",
        "    batch_size=args.batch_size\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRfF5R9MxPWR"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wiPWrBvxPWR"
      },
      "source": [
        "model = Xception(weights='imagenet', include_top=False)\n",
        "model = add_new_last_layer(model, nb_classes)\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRMl52NsxPWR"
      },
      "source": [
        "## Training\n",
        "\n",
        "### TODO: Perform training (transfer learning and fine tuning) to get the best results you can.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QX41-YUPxPWS"
      },
      "source": [
        "### Transfer learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1FHTyr9xPWS"
      },
      "source": [
        "setup_to_transfer_learn(model, args)\n",
        "\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch = train_steps,\n",
        "    epochs = 5,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps = valid_steps,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WA_8iJBclp7G"
      },
      "source": [
        "tl_history = process_and_display(history)\n",
        "model.save_weights('checkpoints/transfer_learning')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJYC8E99xPWS"
      },
      "source": [
        "### Fine tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FC55raTpxPWT"
      },
      "source": [
        "model.layers[126].get_config()['name']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JETkzkH0xPWT"
      },
      "source": [
        "setup_to_finetune(model, args, not_trainable=126)\n",
        "\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch = train_steps,\n",
        "    epochs = 2,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps = valid_steps,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Get0OWwplp7H"
      },
      "source": [
        "ft126_history = process_and_display(history, tl_history)\n",
        "model.save_weights('checkpoints/fine_tuning_126')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZuNe9u0lp7I"
      },
      "source": [
        "**You should continue unfreezeng the layers and training the model until the model overfits. Feel free to add as many code cells as you wish**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zlEFyHPSlp7I"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rjco8tmklp7I"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8GKnyK7lp7J"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4OlfZhwlp7J"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i0UREepClp7J"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HuPWbYlYxPWT"
      },
      "source": [
        "# Save the final model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Hl0_G2UxPWT"
      },
      "source": [
        "model.load_weights('checkpoints/fine_tuning_126')\n",
        "model.save('checkpoints/final')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0U6BCXTexPWU"
      },
      "source": [
        "# TEST SCORES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xy3Kb8iWxPWU"
      },
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "model = load_model('checkpoints/final')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "deicCzzuxPWU"
      },
      "source": [
        "nb_test_samples = get_nb_files(args.test_dir)\n",
        "test_steps = int (nb_test_samples / args.batch_size)\n",
        "model.evaluate(test_generator, steps=test_steps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBm6F5edxPWU"
      },
      "source": [
        "# TO DO\n",
        "Show examples how your model works with real images. \n",
        "Try to display:\n",
        "- An image\n",
        "- Real class of an image\n",
        "- Estimated class of an image\n",
        "- Probabilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zoZByk6lp7K"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}